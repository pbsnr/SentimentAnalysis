{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering Project 1 (ML Model Building)\n",
    "by Brenden BALANE and Paul BASNIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from nlppreprocess import NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>you should all sit down together and watch the...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>was teens when discovered zen meditation was ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>jesus was zen meets jew</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>there are two varieties christians dogmatic th...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>dont worry about trying explain yourself just ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>recently told family that buddhist live the b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>unto others you would have them unto you woul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>first understand that you are not anyway contr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>recently heard similar question where person ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>different times different cultures same point ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>does evil include the lady pai chunked</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>our campaign has two suns one pelor the other ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>technically you could argue that sobek evil wo...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>zarus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>blood and souls for lord arioch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>always liked vecna the one time was able use ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>though don have any good suggestions for addit...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>cliche but you can wrong with cyric mean was ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>zon kuthon and his sister shelyn good deity ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>homebrew setting the primary lawful evil deit...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>his name shall lump wherever lumps are there w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>for one campaign pretty much just snatched the...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>demogorgon because fuck you and your shit god</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>you want badasss evil gods you want real gods ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>pelor the burning hate the best evil god</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label\n",
       "0    family mormon have never tried explain them t...      1\n",
       "1   buddhism has very much lot compatible with chr...      1\n",
       "2   seriously don say thing first all they won get...     -1\n",
       "3   what you have learned yours and only yours wha...      0\n",
       "4   for your own benefit you may want read living ...      1\n",
       "5   you should all sit down together and watch the...     -1\n",
       "6    was teens when discovered zen meditation was ...      1\n",
       "7                            jesus was zen meets jew       0\n",
       "8   there are two varieties christians dogmatic th...     -1\n",
       "9   dont worry about trying explain yourself just ...      1\n",
       "10   recently told family that buddhist live the b...      1\n",
       "11   unto others you would have them unto you woul...      1\n",
       "12  first understand that you are not anyway contr...      1\n",
       "13   recently heard similar question where person ...      1\n",
       "14  different times different cultures same point ...      1\n",
       "15            does evil include the lady pai chunked      -1\n",
       "16  our campaign has two suns one pelor the other ...      1\n",
       "17  technically you could argue that sobek evil wo...     -1\n",
       "18                                             zarus       0\n",
       "19                   blood and souls for lord arioch       0\n",
       "20   always liked vecna the one time was able use ...      1\n",
       "21  though don have any good suggestions for addit...     -1\n",
       "22   cliche but you can wrong with cyric mean was ...     -1\n",
       "23  zon kuthon and his sister shelyn good deity ha...      1\n",
       "24   homebrew setting the primary lawful evil deit...     -1\n",
       "25  his name shall lump wherever lumps are there w...      0\n",
       "26  for one campaign pretty much just snatched the...     -1\n",
       "27     demogorgon because fuck you and your shit god      -1\n",
       "28  you want badasss evil gods you want real gods ...     -1\n",
       "29          pelor the burning hate the best evil god      -1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading data\n",
    "#Reddit Sentimental analysis Dataset by Chaithanya Kumar A\n",
    "df = pd.read_csv('Reddit_Data.csv', encoding='latin1')\n",
    "df.columns = ['text', 'label']\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37249"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "-1     8277\n",
       " 0    13142\n",
       " 1    15830\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercasing every text inputs\n",
    "df['text'] = df.text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokenization\n",
    "df['text'] = [word_tokenize(str(entry)) for entry in df['text'].dropna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "word_Lemmatized = WordNetLemmatizer()\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "for index,entry in enumerate(df['text']):\n",
    "    final_words = []\n",
    "    \n",
    "    for word,tag in pos_tag(entry):\n",
    "        if word.isalpha():\n",
    "            #print(word)\n",
    "            final_words.append(word_Lemmatized.lemmatize(word,tag_map[tag[0]]))\n",
    "            \n",
    "    df.loc[index,'text_final'] = str(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data splitting for training and testing the model\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(df['text_final'],df['label'],test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "train_y_encoded = encoder.fit_transform(train_y)\n",
    "test_y_encoded = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[family, mormon, have, never, tried, explain, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['family', 'mormon', 'never', 'try', 'explain'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[buddhism, has, very, much, lot, compatible, w...</td>\n",
       "      <td>1</td>\n",
       "      <td>['buddhism', 'much', 'lot', 'compatible', 'chr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[seriously, don, say, thing, first, all, they,...</td>\n",
       "      <td>-1</td>\n",
       "      <td>['seriously', 'say', 'thing', 'first', 'get', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[what, you, have, learned, yours, and, only, y...</td>\n",
       "      <td>0</td>\n",
       "      <td>['learn', 'want', 'teach', 'different', 'focus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[for, your, own, benefit, you, may, want, read...</td>\n",
       "      <td>1</td>\n",
       "      <td>['benefit', 'may', 'want', 'read', 'live', 'bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  [family, mormon, have, never, tried, explain, ...      1   \n",
       "1  [buddhism, has, very, much, lot, compatible, w...      1   \n",
       "2  [seriously, don, say, thing, first, all, they,...     -1   \n",
       "3  [what, you, have, learned, yours, and, only, y...      0   \n",
       "4  [for, your, own, benefit, you, may, want, read...      1   \n",
       "\n",
       "                                          text_final  \n",
       "0  ['family', 'mormon', 'never', 'try', 'explain'...  \n",
       "1  ['buddhism', 'much', 'lot', 'compatible', 'chr...  \n",
       "2  ['seriously', 'say', 'thing', 'first', 'get', ...  \n",
       "3  ['learn', 'want', 'teach', 'different', 'focus...  \n",
       "4  ['benefit', 'may', 'want', 'read', 'live', 'bu...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(r'df_sentiment_analysis.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text vectorization\n",
    "tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "tfidf_vect.fit(df['text_final'])\n",
    "train_x_tfidf = tfidf_vect.transform(train_x)\n",
    "test_x_tfidf = tfidf_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  84.69798657718121\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(train_x_tfidf,train_y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(test_x_tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, test_y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the model\n",
    "speech_to_classify = \"i love Paul\"\n",
    "SVM.predict(tfidf_vect.transform([speech_to_classify]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model to disk\n",
    "sentiment_classifier = 'sentiment_analysis_model.sav'\n",
    "pickle.dump(SVM, open(sentiment_classifier, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading pre-trained model and testing it\n",
    "loaded_model = pickle.load(open('sentiment_analysis_model.sav', 'rb'))\n",
    "loaded_model.predict(tfidf_vect.transform([speech_to_classify]))[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
